#This file is a part of g4mice
#
#g4mice is free software: you can redistribute it and/or modify
#it under the terms of the GNU General Public License as published by
#the Free Software Foundation, either version 3 of the License, or
#(at your option) any later version.
#
#g4mice is distributed in the hope that it will be useful,
#but WITHOUT ANY WARRANTY; without even the implied warranty of
#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#GNU General Public License for more details.
#
#You should have received a copy of the GNU General Public License
#along with g4mice in the doc folder.  If not, see 
#<http://www.gnu.org/licenses/>.
#

"""
Framework for Application-level tests for G4MICE

Chris Rogers, 22/11/2010

See also documentation at http://micewww.pp.rl.ac.uk:8080/projects/g4mice/wiki/ApplicationTests

Application-level tests for G4MICE (and other monte carlo codes) usually involve running an application against some geometry files and then running a set
of tests on the output. CodeComparison provides a framework for these sorts of tests. In CodeComparison we define a geometry object and associate with each 
geometry object a set of test objects. 

* The geometry object knows enough to set-up a geometry and any other input data and run the code. Because it's faster, the geometry also reads the output.
* The tests know enough to read in any output files from the application, parse them appropriately and calculate appropriate test variables on the output
* eval( repr(geometry) ) should recreate a copy of the geometry. Then storage of reference datasets and geometries is done by writing repr(geometry) to disk

In this file I define

* geometry = geometry object for running simulation code as described above
* test     = test abstract data type
* ks_test  = implementation of test that does a Kolmogorov-Smirnov test of some user-defined list of variables against some bunch
* hit_equality_test = test if two hits are equal (within some floating point tolerance)

This can be run as a standalone program from the command line, in which case it takes two arguments

  --in=<some_file> defines input file which contains reference geometry objects and tests
  --out=<some_file> defines output file; will write updated reference data to out

"""

#TODO: 
# * at the moment the read of simulation data is done at geometry level, because this is quicker. But some tests may want to do it per test, so this
#   option should be enabled.
# * add some more test types; moments, Twiss parameters
# * it's uncomfortable that I open the file and do eval(<entire_file>) - can lead to stability issues and a pain to debug. This should probably be improved
# * what happens if I want to do a test that crosses geometries? For example, check that within a particular version step size doesn't affect material
#   model. Solution - add a test manager class that holds map of (geo_1, geo_2,...):(test_1, test_2, ...)??? Not sure

#Note: I always use a static function to initialise objects so that I can deal with wanting more than one constructor with the same number of parameters

import xboa.Bunch
import xboa.Hit
from xboa.Hit import Hit
from xboa.Bunch import Bunch
import xboa.Common as Common
import sys
import os
import copy
import ctypes
import ROOT
import exceptions
import math

class geometry:
  """
  Geometry object knows enough to run any monte carlo/whatever code that are generated by that geometry and parse the output to create a Bunch used for 
  tests. Geometries are stored as a set of text files that drive the application and a list of text substitutions that are applied to the text files. The
  substitutions are there to allow easier manipulation of families of geometries, e.g. where we have one common text file with a few words changed each
  time.

  I've set it up so that eval(repr(geometry) ) really does do a copy of the geometry and tests. I thought this would be useful - this way you can do 
  repr(geometry) to write out the geometry and all tests and reference data. Then eval() will recover all tests and reference data. You can see that the 
  code to actually run a test is pretty minimal (there is an example at the end of this file). The downside is that the eval(string) step is hard to debug.

    name         = human-readable string name used to identify the geometry
    code_model   = tuple of (system_call used to run the code; list of command line arguments; dict of file_in:file_out files required for substitution)
    subsitutions = set of substitutions to make into input files to define the accelerator geometry
    bunch_read   = tuple of arguments to Bunch.new_dict_from_read_builtin(...) to load bunches used for tests; typically tuple like (file_type, file_name)
    bunch_index  = index of output bunch used for all tests. If set to 'All', just reads all hits.
    tests        = list of tests that will be run against the bunch
  """
  def __init__(self):
    self.name           = ''
    self.code_model     = ('',[],{})
    self.substitutions  = {}
    self.bunch_index    = 0
    self.bunch_read     = ('', '')
    self.tests          = []

  def __repr__(self):
    """
    Write out the geometry object. You should really be able to do geometry_object == eval(repr(geometry_object)).
    """
    return 'geometry.new('+repr(self.name)+','+repr(self.code_model)+','+repr(self.substitutions)+','+repr(self.bunch_index)+','+repr(self.bunch_read)+','+repr(self.tests)+')'

  def __str__(self):
    """
    Write out a summary of geometry object information.
    """
    out = str(self.name)+'\n'
    for test in self.tests: out += str(test)+'\n'
    return out

  def new(name, code_model, substitutions, bunch_index, bunch_read, tests):
    """
    Initialise the object
    """
    my_geo = geometry()
    my_geo.name          = name
    my_geo.code_model    = code_model
    my_geo.substitutions = substitutions
    my_geo.bunch_index   = bunch_index
    my_geo.bunch_read    = bunch_read
    my_geo.tests         = tests
    return my_geo
  new = staticmethod(new)

  def deepcopy(self):
    """
    Return a copy of the object
    """
    return geometry.new(self.name, self.code_model, self.substitutions, self.bunch_index, self.bunch_read, self.tests)


  def read_bunch(self):
    """
    Read the input file
    """
    if self.bunch_index != 'all':
      return Bunch.new_dict_from_read_builtin(*self.bunch_read)[self.bunch_index]
    else:
      return Bunch.new_from_read_builtin(*self.bunch_read)


  def run_mc(self):
    """
    Set-up auxiliary files and run executable
    """
    (executable, command_line_args, auxiliary_files) = self.code_model
    for fin,fout in auxiliary_files.iteritems():
      Common.substitute(fin, fout, self.substitutions)
    if command_line_args != None:
      for arg in command_line_args: executable += ' '+arg
    os.system(executable)

  def run_tests(self):
    """
    Run all the tests associated with this geometry
    """
    (passes,fails,warns) = (0,0,0)
    self.run_mc()
    test_bunch = self.read_bunch()
    for i,test in enumerate(self.tests):
      test_out = test.run_test(test_bunch)
      if   test_out.test_result() == 'pass': passes +=1
      elif test_out.test_result() == 'fail': fails  +=1
      else:                    warns  +=1
      self.tests[i] = test_out
    print self
    return (passes, fails, warns)

#####################

class test:
  """
  Test object is an abstract type that contains enough information to run a test against some bunch of particle data
  """
  def __init__(self):
    """Initialise the test to 0."""
    raise(NotImplementedError(__name__+" not implemented for this test class"))

  def __repr__(self):
    """Represent the test as a string. Should enforce some_test == eval(repr(some_test))"""
    raise(NotImplementedError(__name__+" not implemented for this test class"))

  def __str__(self):
    """Summary information on the test"""
    raise(NotImplementedError(__name__+" not implemented for this test class"))

  def new(): 
    """Initialise a new test."""
    raise(NotImplementedError(__name__+" not implemented for this test class"))
  new = staticmethod(new)

  def deepcopy():
    """Make a copy of the test, copying memory over also. """
    raise(NotImplementedError(__name__+" not implemented for this test class"))

  def run_test(self, bunch):
    """
    Returns a new reference test object whose internal state reflects the data in the bunch
    """
    raise(NotImplementedError(__name__+" not implemented for this test class"))

  def test_result(self):
    """
    Return 'pass', 'fail' or 'warn' to indicate whether the test passed, failed, or generated a warning (e.g. library not available for testing)
    """
    raise(NotImplementedError(__name__+" not implemented for this test class"))


  def make_plots(test_list):
    """
    Make plots if appropriate from each of the tests in test_list. Else throw NotImplemented.
    """
    raise(NotImplementedError(__name__+" not implemented for this test class"))
  make_plots = staticmethod(make_plots)

####################

class ks_test(test): #note inheritance from test
  """
  ks test object is a summary of information generated and used for ks test. Idea is to make a summary information such that we don't need to store round large datasets to test against.
    variable = string name of the (Hit) variable for testing. Should be one of Hit.get() type variables.
    units    = string name of the units the variable is stored in
    bins     = list of bin edges
    content  = numpy array of bin contents (weight)
    n_events = number of events that was used to generate the data
    tolerance = ks tolerance (probability); below this tolerance, test is presumed to have failed
    ks_dist  = calculated maximum kolmogorov-smirnov distance
    ks_prob  = calculated kolmogorov-smirnov probability that the ks test is the same as some reference data
  """
  def __init__(self):
    self.variable  = ''
    self.units     = ''
    self.bins      = []
    self.content   = []
    self.n_events  = 0
    self.pid       = -13
    self.ks_dist   = 0.
    self.ks_prob   = 0.
    self.ks_tol    = 1.
  
  def __repr__(self):
    return 'ks_test.new('+repr(self.variable)+','+repr(self.units)+','+repr(self.bins)+','+repr(self.content)+','+repr(self.n_events)+','+repr(self.pid)+','+repr(self.ks_dist)+','+repr(self.ks_prob)+','+repr(self.ks_tol)+')'

  def __str__(self):
    """Summary of the KS test along with test pass information"""
    return 'ks test with variable '+str(self.variable)+' ks distance '+str(self.ks_dist)+' probability '+str(self.ks_prob)+' tolerance '+str(self.ks_tol)+' : '+str(self.test_result())

  def new(variable, units, bins, content, n_events, pid, ks_dist, ks_prob, ks_tol):
    """
    Initialises the object
    """
    my_ks_test = ks_test()
    my_ks_test.variable  = copy.deepcopy(variable)
    my_ks_test.units     = copy.deepcopy(units)
    my_ks_test.bins      = copy.deepcopy(bins)
    my_ks_test.content   = copy.deepcopy(content)
    my_ks_test.n_events  = copy.deepcopy(n_events)
    my_ks_test.pid       = copy.deepcopy(pid)
    my_ks_test.ks_dist   = copy.deepcopy(ks_dist)
    my_ks_test.ks_prob   = copy.deepcopy(ks_prob)
    my_ks_test.ks_tol    = copy.deepcopy(ks_tol)
    return my_ks_test
  new = staticmethod(new)

  def deepcopy(self):
    """
    Return a copy of the object
    """
    return ks_test.new(self.variable, self.units, self.bins, self.n_events, self.content, self.pid, self.ks_dist, self.ks_prob, self.ks_tol)

  def run_test(self, test_bunch):
    """
    Run the ks test on the bunch. Return value is a new ks_test with internal state updated to reflect new test data
    """
    hist   = test_bunch.histogram_var_bins(self.variable, self.bins, self.units)
    ks_test_out = self.deepcopy()
    ks_test_out.n_events = len(test_bunch)
    ks_test_out.content  = [0.]
    for i in hist[0]: ks_test_out.content.append(i[0]+ks_test_out.content[-1])
    del ks_test_out.content[0]
    if ks_test_out.content[-1] == 0: ks_test_out_content[-1] = 1.
    for i in range(len(ks_test_out.content)): ks_test_out.content[i] /= ks_test_out.content[-1] #normalise to 1
    ks_test_out.ks_dist = 0.
    for i in range( len(ks_test_out.content) ):
      ks_dist = abs(ks_test_out.content[i] - self.content[i])
      if ks_dist > ks_test_out.ks_dist: ks_test_out.ks_dist = ks_dist
    if ks_test_out.n_events+self.n_events==0: 
      ks_dist_mod=999.
    else:
      ks_dist_mod = ks_test_out.ks_dist*((ks_test_out.n_events*self.n_events)/(ks_test_out.n_events+self.n_events))**0.5
    ks_test_out.ks_prob = ROOT.TMath.KolmogorovProb(ks_dist_mod)
    return ks_test_out

  def test_result(self):
    """Return whether the test passed or failed"""
    if self.ks_prob < self.ks_tol: return 'fail'
    else:                          return 'pass'

  def cdf_function(contents):
    """helper function for make_plot. Set to make a cumulative density function plot"""
    return contents
  cdf_function = staticmethod(cdf_function)
      
  def pdf_function(c_in):
    """helper function for make_plot. Set to make a probability density function plot."""
    c_out = [c_in[0]]
    for i in range(1, len(c_in)): c_out.append(c_in[i] - c_in[i-1])
    c_max = max(c_out)
    for i in range(len(c_out)): c_out[i]/=c_max
    return c_out
  pdf_function = staticmethod(pdf_function)

  def hist_width(ks_test_list):
    i = 0
    return lower,upper
  hist_width = staticmethod(hist_width)

  def make_plots(ks_test_list, bin_function=pdf_function, _hist_width=hist_width):
    """Plot the distributions used for the ks test. bin_function describes a function that is performed on the bins before plotting."""
    h_start = len(Common._hist_persistent)+1
    name = ks_test_list[0].variable
    if ks_test_list[0].units!='': name+=' ['+ks_test_list[0].units+']'
    option = ''
    canv = Common.make_root_canvas(name)
    h_list = []
    bad_colors = [5,10] #5 is yellow, 10 is white

    (lower, upper) = -1,-1
    for ks_test in ks_test_list:
      index = 0
      data_max = max(ks_test.content) #I think this is 1. but let's be robust
      while ks_test.content[index] < 0.01*data_max and index<len(ks_test.content): index+=1
      if ks_test.bins[index] < lower or lower < 0: lower = ks_test.bins[index]
      while ks_test.content[index] < 0.99*data_max and index<len(ks_test.content): index+=1
      if ks_test.bins[index] > upper or upper < 0: upper = ks_test.bins[index+1]
    print lower,upper

    h = ROOT.TH1D(name+'-'+str(len(Common._hist_persistent)),';'+name, 10000, lower, upper)
    h.SetLineColor(10)
    h.SetStats(False)
    h.Draw()
    Common._hist_persistent.append(h)
    for k,test in  enumerate(ks_test_list):
      bins = test.bins
      bin_array = ctypes.c_double*len(bins)
      ba = bin_array()
      for i in range(len(bins)): ba[i]=ctypes.c_double(bins[i])
      h = ROOT.TH1D(name+'-'+str(len(Common._hist_persistent)),';'+name, len(bins)-1, ba)
      if test.content[-1] != 0.:
        c_out = bin_function(test.content)
        for i,c in enumerate(c_out): h.SetBinContent(i+1, c)
      Common._hist_persistent.append(h)
      color = k+1
      while color in bad_colors: color += 1
      h.SetLineColor(color)
      h.SetStats(False)
      h.Draw('same')
    canv.Update()
    return (canv,h_start,len(Common._hist_persistent))
  make_plots = staticmethod(make_plots)

#######################

class hit_equality_test(test):
  """
  Hit equality test checks that two hits are equal within some floating point tolerance.
    bunch           = check that hits in the bunch are the same as those in the test bunch
    float_tolerance = floating point tolerance for the test
    test_out        = string, either 'pass' or 'fail' depending on whether test passed or failed
  """
  def __init__(self):
    """Initialise an empty equality test"""
    self.bunch     = Bunch()
    self.tolerance = xboa.Common.float_tolerance    
    self.test_out  = 'fail'

  def __repr__(self):
    """Return a string; eval(repr(test) ) will return an identical test"""
    return 'hit_equality_test.new('+repr(self.tolerance)+','+repr(self.test_out)+','+repr(self.bunch)+')'

  def __str__(self):
    return 'hit_equality_test with bunch of length '+str(len(self.bunch))+' with tolerance: '+str(self.tolerance)+' result: '+str(self.test_out)

  def new(tolerance, test_out, bunch):
    """initialise a new test"""
    _test = hit_equality_test()
    _test.bunch = bunch
    _test.tolerance = tolerance
    _test.test_out = test_out
    return _test 
  new = staticmethod(new)

  def run_test(self, test_bunch):
    """Compare test_bunch with stored bunch. return an updated copy of self with test bunch and """
    het_out = copy.deepcopy(self)
    het_out.test_out = 'pass'
    het_out.bunch       = test_bunch
    if not Bunch.__eq__(self.bunch, test_bunch,self.tolerance):
        het_out.test_out = 'fail'
    return het_out

  def test_result(self):
    """Return test result ('pass' or 'fail') depending on whether the test passed or failed"""
    return self.test_out

  def deepcopy(self):
    """Return a copy of self (allocating new memory)"""
    return hit_equality_test.new(self.tolerance, self.test_out, self.bunch)

  def make_plot(self):
    """No plots to make - so does nothing"""
    pass

#######################

class field_equality_test(test):
  """
  A type of hit_equality_test. In this case, we only test E-field, B-field, position and time of test particles
  """
  def __init__(self):
    """Initialise an empty equality test"""
    self.bunch     = Bunch()
    self.tolerance = xboa.Common.float_tolerance    
    self.test_out  = 'fail'


  def __repr__(self):
    """Return a string; eval(repr(test) ) will return an identical test"""
    return 'field_equality_test.new('+repr(self.tolerance)+','+repr(self.test_out)+','+repr(self.bunch)+')'

  def __str__(self):
    return 'field_equality_test with bunch of length '+str(len(self.bunch))+' with tolerance: '+str(self.tolerance)+' result: '+str(self.test_out)

  def new(tolerance, test_out, bunch):
    """initialise a new test"""
    _test = field_equality_test()
    _test.bunch = bunch
    _test.tolerance = tolerance
    _test.test_out = test_out
    return _test 
  new = staticmethod(new)

  def run_test(self, test_bunch):
    """Compare test_bunch with stored bunch. return an updated copy of self with test bunch"""
    het_out = copy.deepcopy(self)
    het_out.test_out = 'pass'
    het_out.bunch       = test_bunch
    if len(self.bunch) != len(test_bunch):
      het_out.test_out = 'fail'
      return het_out
    tests = ['x', 'y', 'z', 'bx', 'by', 'bz']
    for i,hit in enumerate(self.bunch):
      for t in tests:
        if abs(self.bunch[i][t]-test_bunch[i][t]) > self.tolerance:
          het_out.test_out = 'fail'
    return het_out

  def test_result(self):
    """Return test result ('pass' or 'fail') depending on whether the test passed or failed"""
    return self.test_out

  def deepcopy(self):
    """Return a copy of self (allocating new memory)"""
    return hit_equality_test.new(self.tolerance, self.test_out, self.bunch)

  def make_plot(self):
    """No plots to make - so does nothing"""
    pass

#######################

def read_geometries(ref_data_in):
  """
  Returns a list of geometry objects generated from a reference data set
    ref_data_in = the file name (string)
  """
  try:    fin = open(ref_data_in)
  except: raise(IOError('Failed to open file '+str(ref_data_in)))
  geometries = eval(fin.read())
  return geometries

def write_geometries(ref_data_out, geometries):
  """
  Writes a list of geometry objects generated from a reference data set
    ref_data_out = the file name (string)
    geometries   = write the geometries
  """
  if ref_data_out != None: print 'Writing output to '+ref_data_out
  fout = open(ref_data_out, 'w')
  print >>fout,repr(geometries)
  fout.close()

def run_tests(geometries_in, geometries_out):
  """
  Run all the geometries in the list
    geometry_list = list of geometry objects
  """
  (passes,fails,warns)=(0,0,0)
  for geo_in in geometries_in: 
    try:
      geo_out = geo_in.deepcopy()
      (p,f,w) = geo_out.run_tests()
      passes += p
      fails  += f
      warns  += w
      geometries_out.append(geo_out)
      sys.stdout.flush()
    except:
      print 'Caught exception in geometry ',geo_in.name
      sys.excepthook(*sys.exc_info())
      if sys.exc_info()[0] == exceptions.KeyboardInterrupt: raise exceptions.KeyboardInterrupt
  return (passes,fails,warns)


def code_comparison_test(ref_data_in, ref_data_out):
  """
  Main loop:- parses the input file and calls functions to run the tests and write output
  """
  (passes, fails, warns) = (0,0,0)
  geometries_out = []
  fin  = open(ref_data_in)
  (passes,fails,warns) = run_tests(eval(fin.read()), geometries_out)  
  write_geometries(ref_data_out, geometries_out)
  for geo in geometries_out: print geo
  return (passes,fails,warns)

def main(argv=None):
  """
  If called directly from the command line, parse command line arguments as file names and run the tests.
  """
  if argv==None: argv=sys.argv
  arg_dict = {}
  for arg in argv:
    if( len(arg.split('='))>1 ):
      arg_dict[arg.split('=')[0]] = arg.split('=')[1]
  arg_list = ['--in','--out']
  args = []
  for arg in arg_list:
    if arg in arg_dict: args.append(arg_dict[arg])
    else:               args.append(None)
  (passes,fails,warns) = code_comparison_test(*args)
  print '========================='
  print '|| CodeComparisonTest  ||'
  print '========================='
  print '\n'+str(passes)+' passes   '+str(fails)+' fails    '+str(warns)+' warnings\n'
  return fails

if __name__ == "__main__":
  sys.exit(main())



